These are my codes.... import { useState, useEffect, useRef } from 'react';
import { FaPhone, FaPhoneSlash, FaPaperPlane, FaSpinner, FaPlayCircle, FaPauseCircle, FaMicrophone } from 'react-icons/fa';
import axios from 'axios';
import { motion, AnimatePresence } from 'framer-motion';

const Chatbot = ({ isPanelOpen, voiceModel = 'female' }) => {
  const [messages, setMessages] = useState([]);
  const [input, setInput] = useState('');
  const [isInCall, setIsInCall] = useState(false);
  const [mediaRecorder, setMediaRecorder] = useState(null);
  const [websocket, setWebsocket] = useState(null);
  const chatEndRef = useRef(null);
  const [isSending, setIsSending] = useState(false);
  const [isBotTyping, setIsBotTyping] = useState(false);
  const [currentAudio, setCurrentAudio] = useState(null);
  const [playingMessageId, setPlayingMessageId] = useState(null);
  const [isPlaying, setIsPlaying] = useState(false);
  const audioChunksRef = useRef([]);
  const streamRef = useRef(null);
  const audioContextRef = useRef(null);
  const analyserRef = useRef(null);
  const wsRetryCount = useRef(0);
  const maxWsRetries = 3;

  useEffect(() => {
    const timer = setTimeout(() => {
      chatEndRef.current?.scrollIntoView({ behavior: 'smooth' });
    }, 100);
    return () => clearTimeout(timer);
  }, [messages]);

  useEffect(() => {
    return () => {
      if (currentAudio) {
        currentAudio.pause();
        currentAudio.currentTime = 0;
        setCurrentAudio(null);
        setPlayingMessageId(null);
        setIsPlaying(false);
      }
      if (websocket && websocket.readyState === WebSocket.OPEN) {
        websocket.close();
        setWebsocket(null);
      }
      if (mediaRecorder && mediaRecorder.state !== 'inactive') {
        mediaRecorder.stop();
        setMediaRecorder(null);
      }
      if (streamRef.current) {
        streamRef.current.getTracks().forEach((track) => track.stop());
        streamRef.current = null;
      }
      if (audioContextRef.current) {
        audioContextRef.current.close();
        audioContextRef.current = null;
      }
    };
  }, []);

  const stopCurrentAudio = () => {
    if (currentAudio) {
      currentAudio.pause();
      currentAudio.currentTime = 0;
      setCurrentAudio(null);
      setPlayingMessageId(null);
      setIsPlaying(false);
    }
  };

  const handleSendMessage = async (queryText) => {
    if (!queryText.trim() || isSending) return;

    setIsSending(true);
    const userMessage = { role: 'user', content: queryText };
    setMessages((prev) => [...prev, userMessage]);
    setInput('');
    setIsBotTyping(true);

    try {
      console.log('Sending query with voice model:', voiceModel);
      const response = await axios.post('http://localhost:8000/query', { query: queryText, voice_model: voiceModel });
      const botContent = response.data?.response || 'No response content received.';
      const audioUrl = response.data?.audio_url;

      const botMessage = { role: 'assistant', content: botContent, audioUrl };
      setMessages((prev) => [...prev, botMessage]);
    } catch (error) {
      console.error('Error sending message:', error);
      const errorMessage = {
        role: 'assistant',
        content: 'Error: ' + (error.response?.data?.detail || error.message || 'An unexpected error occurred.'),
      };
      setMessages((prev) => [...prev, errorMessage]);
    } finally {
      setIsSending(false);
      setIsBotTyping(false);
    }
  };

  const handlePlayAudio = (audioUrl, messageId) => {
    if (!audioUrl) return;

    const fullAudioUrl = audioUrl.startsWith('http') ? audioUrl : `http://localhost:8000${audioUrl}`;
    console.log('Handling audio for:', fullAudioUrl);

    if (playingMessageId === messageId && currentAudio) {
      if (isPlaying) {
        currentAudio.pause();
        setIsPlaying(false);
      } else {
        currentAudio.play().catch((e) => {
          console.error('Audio playback failed:', e);
          setMessages((prev) => [
            ...prev,
            { role: 'assistant', content: 'Error playing audio: ' + e.message },
          ]);
        });
        setIsPlaying(true);
      }
      return;
    }

    stopCurrentAudio();

    const audio = new Audio(fullAudioUrl);
    setCurrentAudio(audio);
    setPlayingMessageId(messageId);
    setIsPlaying(true);

    audio.oncanplay = () => {
      console.log('Audio is ready to play');
      audio.play().catch((e) => {
        console.error('Audio playback failed:', e);
        setMessages((prev) => [
          ...prev,
          { role: 'assistant', content: 'Error playing audio: ' + e.message },
        ]);
        setCurrentAudio(null);
        setPlayingMessageId(null);
        setIsPlaying(false);
      });
    };

    audio.onended = () => {
      setCurrentAudio(null);
      setPlayingMessageId(null);
      setIsPlaying(false);
    };

    audio.onerror = (e) => {
      console.error('Audio loading error:', e);
      setMessages((prev) => [
        ...prev,
        { role: 'assistant', content: 'Error loading audio: ' + e.message },
      ]);
      setCurrentAudio(null);
      setPlayingMessageId(null);
      setIsPlaying(false);
    };

    audio.load();
  };

  const handleTestMicrophone = async () => {
    try {
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
      const recorder = new MediaRecorder(stream, { mimeType: 'audio/webm;codecs=opus' });
      const chunks = [];

      recorder.ondataavailable = (e) => {
        if (e.data.size > 0) {
          chunks.push(e.data);
          console.log('Test chunk captured:', e.data.size);
        } else {
          console.warn('Test chunk empty');
        }
      };

      recorder.onstop = () => {
        const blob = new Blob(chunks, { type: 'audio/webm;codecs=opus' });
        const url = URL.createObjectURL(blob);
        const a = document.createElement('a');
        a.href = url;
        a.download = `test_recording_${Date.now()}.webm`;
        a.click();
        URL.revokeObjectURL(url);
        stream.getTracks().forEach((track) => track.stop());
        console.log('Test recording saved');
      };

      recorder.start(1000);
      setTimeout(() => {
        if (recorder.state !== 'inactive') {
          recorder.stop();
        }
      }, 5000);
      console.log('Recording test audio for 5s...');
    } catch (e) {
      console.error('Microphone test error:', e);
      setMessages((prev) => [
        ...prev,
        { role: 'assistant', content: 'Error testing microphone: ' + e.message },
      ]);
    }
  };

  const handleStartCall = async () => {
    if (isInCall) return;

    setIsInCall(true);
    wsRetryCount.current = 0;

    const setupWebSocket = () => {
      const ws = new WebSocket('ws://localhost:8000/ws/transcribe');
      ws.onopen = () => {
        console.log('WebSocket connection established, readyState:', ws.readyState);
        wsRetryCount.current = 0;
        setWebsocket(ws);
      };
      ws.onmessage = (event) => {
        try {
          const data = JSON.parse(event.data);
          const transcription = data.transcription;
          if (transcription) {
            console.log('Received transcription:', transcription);
            setInput((prev) => prev + (prev ? ' ' : '') + transcription);
          } else {
            console.warn('Empty transcription received');
            setMessages((prev) => [
              ...prev,
              { role: 'assistant', content: 'Warning: No speech detected in audio chunk.' },
            ]);
          }
        } catch (e) {
          console.error('Error parsing WebSocket message:', e);
          setMessages((prev) => [
            ...prev,
            { role: 'assistant', content: 'Error processing transcription: ' + e.message },
          ]);
        }
      };
      ws.onerror = (error) => {
        console.error('WebSocket error:', error);
        setMessages((prev) => [
          ...prev,
          { role: 'assistant', content: 'Error in WebSocket connection: ' + error.message },
        ]);
      };
      ws.onclose = (event) => {
        console.log('WebSocket closed:', event.code, event.reason);
        setWebsocket(null);
        if (isInCall && wsRetryCount.current < maxWsRetries) {
          console.log('Retrying WebSocket connection, attempt:', wsRetryCount.current + 1);
          wsRetryCount.current += 1;
          setTimeout(setupWebSocket, 1000);
        } else {
          setIsInCall(false);
          if (mediaRecorder && mediaRecorder.state !== 'inactive') {
            mediaRecorder.stop();
          }
        }
      };
      return ws;
    };

    try {
      // Initialize WebSocket
      const ws = setupWebSocket();

      // Initialize MediaRecorder
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
      streamRef.current = stream;
      console.log('Microphone stream active:', stream.active);
      stream.getTracks().forEach((track) => {
        console.log('Track state:', track.kind, track.readyState, track.enabled);
      });

      // Setup AudioContext
      audioContextRef.current = new (window.AudioContext || window.webkitAudioContext)();
      console.log('AudioContext state:', audioContextRef.current.state);
      const source = audioContextRef.current.createMediaStreamSource(stream);
      analyserRef.current = audioContextRef.current.createAnalyser();
      analyserRef.current.fftSize = 2048;
      source.connect(analyserRef.current);
      const dataArray = new Uint8Array(analyserRef.current.fftSize);
      const checkAudioLevel = () => {
        analyserRef.current.getByteTimeDomainData(dataArray);
        const max = Math.max(...dataArray);
        const min = Math.min(...dataArray);
        console.log('Audio input level: max=', max, 'min=', min);
        if (max > 138) {
          console.log('Significant audio detected');
        }
        if (isInCall && audioContextRef.current.state === 'running') {
          requestAnimationFrame(checkAudioLevel);
        }
      };
      requestAnimationFrame(checkAudioLevel);

      let mimeType = 'audio/webm;codecs=opus';
      if (!MediaRecorder.isTypeSupported(mimeType)) {
        console.warn('audio/webm;codecs=opus not supported, falling back to audio/webm');
        mimeType = 'audio/webm';
        if (!MediaRecorder.isTypeSupported(mimeType)) {
          console.error('No supported audio MIME type found for MediaRecorder.');
          setMessages((prev) => [
            ...prev,
            { role: 'assistant', content: 'Error: Your browser does not support audio recording.' },
          ]);
          ws.close();
          stream.getTracks().forEach((track) => track.stop());
          audioContextRef.current.close();
          setIsInCall(false);
          return;
        }
      }
      console.log('Using MIME type:', mimeType);

      const options = { mimeType, timeslice: 1000 };
      const recorder = new MediaRecorder(stream, options);
      audioChunksRef.current = [];

      recorder.onstart = () => {
        console.log('MediaRecorder started, state:', recorder.state);
      };

      recorder.ondataavailable = (e) => {
        console.log('Data available, chunk size:', e.data.size);
        if (e.data.size > 0) {
          console.log('Captured audio chunk:', e.data.size);
          audioChunksRef.current.push(e.data);
          if (ws.readyState === WebSocket.OPEN) {
            const blob = new Blob(audioChunksRef.current, { type: mimeType });
            blob.arrayBuffer().then((buffer) => {
              console.log('Sending audio chunk:', blob.size);
              ws.send(buffer);
            }).catch((e) => {
              console.error('Error converting Blob to ArrayBuffer:', e);
              setMessages((prev) => [
                ...prev,
                { role: 'assistant', content: 'Error processing audio chunk: ' + e.message },
              ]);
            });
            audioChunksRef.current = audioChunksRef.current.slice(-1);
          } else {
            console.warn('WebSocket not open, discarding chunk');
          }
        } else {
          console.warn('Empty audio chunk received');
          setMessages((prev) => [
            ...prev,
            { role: 'assistant', content: 'Warning: Empty audio chunk received. Check microphone input.' },
          ]);
        }
      };

      recorder.onstop = () => {
        console.log('MediaRecorder stopped, state:', recorder.state);
        if (streamRef.current) {
          streamRef.current.getTracks().forEach((track) => {
            if (track.readyState === 'live') {
              track.stop();
              console.log('Stopping track:', track.kind);
            }
          });
          streamRef.current = null;
        }
        if (audioContextRef.current) {
          audioContextRef.current.close();
          audioContextRef.current = null;
        }
        if (ws.readyState === WebSocket.OPEN) {
          ws.close();
        }
        audioChunksRef.current = [];
        setMediaRecorder(null);
        setIsInCall(false);
      };

      recorder.onerror = (e) => {
        console.error('MediaRecorder error:', e.error);
        setMessages((prev) => [
          ...prev,
          { role: 'assistant', content: 'Error recording audio: ' + e.error.message },
        ]);
        handleEndCall();
      };

      // Delay recorder start to ensure WebSocket is ready
      setTimeout(() => {
        if (!isInCall) return;
        try {
          recorder.start(options.timeslice);
          console.log('MediaRecorder starting with timeslice:', options.timeslice);
          setMediaRecorder(recorder);
          console.log('Call started, streaming audio');
        } catch (e) {
          console.error('Error starting MediaRecorder:', e);
          setMessages((prev) => [
            ...prev,
            { role: 'assistant', content: 'Error starting audio recording: ' + e.message },
          ]);
          ws.close();
          stream.getTracks().forEach((track) => track.stop());
          audioContextRef.current.close();
          setIsInCall(false);
        }
      }, 1000);

    } catch (error) {
      console.error('Error starting call:', error);
      setMessages((prev) => [
        ...prev,
        { role: 'assistant', content: 'Error accessing microphone: ' + error.message },
      ]);
      setIsInCall(false);
      if (streamRef.current) {
        streamRef.current.getTracks().forEach((track) => track.stop());
        streamRef.current = null;
      }
      if (audioContextRef.current) {
        audioContextRef.current.close();
        audioContextRef.current = null;
      }
      if (websocket) {
        websocket.close();
        setWebsocket(null);
      }
    }
  };

  const handleEndCall = () => {
    if (!isInCall) return;

    if (mediaRecorder && mediaRecorder.state !== 'inactive') {
      mediaRecorder.stop();
    }
    if (websocket && websocket.readyState === WebSocket.OPEN) {
      websocket.close();
    }
    setMediaRecorder(null);
    setWebsocket(null);
    setIsInCall(false);
    audioChunksRef.current = [];
    if (streamRef.current) {
      streamRef.current.getTracks().forEach((track) => track.stop());
      streamRef.current = null;
    }
    if (audioContextRef.current) {
      audioContextRef.current.close();
      audioContextRef.current = null;
    }
    console.log('Call ended');
  };

  return (
    <div className="flex flex-col h-full transition-all duration-300">
      <div className="flex-1 overflow-y-auto p-4 bg-gray-800 rounded-lg shadow-inner border border-gray-700">
        <style>
          {`
            .hide-scrollbar::-webkit-scrollbar {
              display: none;
            }
            .hide-scrollbar {
              scrollbar-width: none;
              -ms-overflow-style: none;
            }
          `}
        </style>
        <AnimatePresence initial={false} mode="sync">
          {messages.map((msg, index) => (
            <motion.div
              key={index}
              className={`mb-6 flex ${msg.role === 'user' ? 'justify-end' : 'justify-start'}`}
              initial={{ opacity: 0, y: 20 }}
              animate={{ opacity: 1, y: 0 }}
              exit={{ opacity: 0, y: -20 }}
              transition={{ duration: 0.3 }}
            >
              <div
                className={`max-w-[75%] p-4 rounded-xl break-words ${
                  msg.role === 'user' ? 'bg-gradient-to-r from-cyan-600 to-blue-700 text-white shadow-lg' : 'bg-gray-700 text-gray-200 shadow-md'
                }`}
                style={{ wordBreak: 'break-word' }}
              >
                {msg.content}
                {msg.audioUrl && msg.role === 'assistant' && (
                  <motion.button
                    onClick={() => handlePlayAudio(msg.audioUrl, index)}
                    className="mt-2 flex items-center text-cyan-400 hover:text-cyan-300 transition duration-200"
                    whileHover={{ scale: 1.05 }}
                    whileTap={{ scale: 0.95 }}
                    aria-label={playingMessageId === index && isPlaying ? 'Pause Audio Response' : 'Play Audio Response'}
                  >
                    {playingMessageId === index && isPlaying ? (
                      <FaPauseCircle className="mr-2" />
                    ) : (
                      <FaPlayCircle className="mr-2" />
                    )}
                    {playingMessageId === index && isPlaying ? 'Pause Response' : 'Play Response'}
                  </motion.button>
                )}
              </div>
            </motion.div>
          ))}
          {isBotTyping && (
            <motion.div
              key="typing-indicator"
              className="mb-6 flex justify-start"
              initial={{ opacity: 0, y: 20 }}
              animate={{ opacity: 1, y: 0 }}
              exit={{ opacity: 0, y: -20 }}
              transition={{ duration: 0.3 }}
            >
              <div className="max-w-[75%] p-4 rounded-xl bg-gray-700 text-gray-400 italic shadow-md flex items-center">
                <FaSpinner className="animate-spin mr-2" />
                Bot is thinking...
              </div>
            </motion.div>
          )}
        </AnimatePresence>
        <div ref={chatEndRef} />
      </div>

      <div className="mt-4 flex items-center space-x-3 p-3 bg-gray-800 rounded-lg shadow-lg border border-gray-700">
        <input
          type="text"
          value={input}
          onChange={(e) => setInput(e.target.value)}
          onKeyPress={(e) => e.key === 'Enter' && handleSendMessage(input)}
          placeholder={isInCall ? 'Transcribing...' : isBotTyping ? 'Waiting for response...' : 'Ask a question...'}
          className={`flex-1 p-3 bg-gray-700 text-white rounded-lg focus:outline-none focus:ring-2 ${
            isInCall ? 'focus:ring-red-500' : 'focus:ring-cyan-500'
          } transition duration-200 placeholder-gray-500`}
          disabled={isSending || isBotTyping}
        />
        <motion.button
          onClick={isInCall ? handleEndCall : handleStartCall}
          className={`p-3 rounded-full transition duration-200 ease-in-out flex items-center justify-center ${
            isInCall ? 'bg-red-600 hover:bg-red-700 animate-pulse' : 'bg-cyan-600 hover:bg-cyan-700'
          }`}
          whileHover={{ scale: 1.1 }}
          whileTap={{ scale: 0.9 }}
          aria-label={isInCall ? 'End Call' : 'Start Call'}
          disabled={isSending || isBotTyping}
        >
          {isInCall ? <FaPhoneSlash className="text-white text-xl" /> : <FaPhone className="text-white text-xl" />}
        </motion.button>
        <motion.button
          onClick={handleTestMicrophone}
          className="p-3 bg-yellow-600 rounded-full hover:bg-yellow-700 transition duration-200 ease-in-out flex items-center justify-center"
          whileHover={{ scale: 1.1 }}
          whileTap={{ scale: 0.9 }}
          aria-label="Test Microphone"
        >
          <FaMicrophone className="text-white text-xl" />
        </motion.button>
        <motion.button
          onClick={() => handleSendMessage(input)}
          className={`p-3 bg-cyan-600 rounded-full hover:bg-cyan-700 transition duration-200 ease-in-out flex items-center justify-center ${
            isSending || !input.trim() || isBotTyping ? 'opacity-50 cursor-not-allowed' : ''
          }`}
          whileHover={{ scale: !input.trim() || isSending || isBotTyping ? 1 : 1.1 }}
          whileTap={{ scale: !input.trim() || isSending || isBotTyping ? 1 : 0.9 }}
          disabled={!input.trim() || isSending || isBotTyping}
          aria-label="Send Message"
        >
          <motion.div
            initial={{ rotate: 0 }}
            animate={{ rotate: isSending ? 360 : 0 }}
            transition={{ duration: 0.5, loop: isSending ? Infinity : 0, ease: 'linear' }}
          >
            <FaPaperPlane className="text-white text-xl" />
          </motion.div>
        </motion.button>
      </div>
    </div>
  );
};

export default Chatbot;

from fastapi import APIRouter, UploadFile, File, HTTPException, Body, WebSocket
from fastapi.responses import JSONResponse, FileResponse
from pydantic import BaseModel
from typing import Dict
from services import convert_audio_to_wav, speech_to_text_streaming, text_to_speech, cleanup_old_audio_files, speech_to_text_websocket
from file_handler import extract_text_from_file
from chunker import chunk_text
from embedder import embed_chunks, store_embeddings
from query_handler import handle_query
from utils import delete_file
from db import get_db
import os
import time
from pathlib import Path
import logging
import psycopg2
import shutil

logger = logging.getLogger(__name__)

router = APIRouter()

MAX_FILE_SIZE = 10 * 1024 * 1024
UPLOAD_DIR = Path("uploads")
AUDIO_DIR = Path("audio")
SIMILARITY_THRESHOLD = 0.5  # Cosine distance threshold (lower is more similar)

class UploadResponse(BaseModel):
    filename: str
    message: str

class AnalyzeResponse(BaseModel):
    filename: str
    chunk_count: int
    message: str

class QueryRequest(BaseModel):
    query: str
    voice_model: str = "female"

class QueryResponse(BaseModel):
    response: str
    audio_url: str

class TranscribeResponse(BaseModel):
    transcribed_text: str

class ClearResponse(BaseModel):
    message: str

@router.post("/upload", response_model=UploadResponse)
async def upload_file(file: UploadFile = File(...)):
    allowed_extensions = {".pdf", ".txt", ".xlsx", ".xls"}
    file_ext = os.path.splitext(file.filename)[1].lower()
    if file_ext not in allowed_extensions:
        logger.warning(f"Unsupported file type uploaded: {file_ext}")
        raise HTTPException(status_code=400, detail="Unsupported file type. Allowed: PDF, TXT, Excel")
    file_size = 0
    for chunk in file.file:
        file_size += len(chunk)
        if file_size > MAX_FILE_SIZE:
            logger.warning(f"File too large: {file.filename}, size: {file_size} bytes")
            raise HTTPException(status_code=400, detail="File too large. Max size: 10MB")
    file.file.seek(0)
    file_path = UPLOAD_DIR / file.filename
    try:
        with open(file_path, "wb") as buffer:
            shutil.copyfileobj(file.file, buffer)
        logger.info(f"File uploaded successfully: {file.filename}")
        return UploadResponse(filename=file.filename, message="File uploaded successfully")
    except Exception as e:
        logger.error(f"Error uploading file {file.filename}: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error uploading file: {str(e)}")

@router.post("/analyze", response_model=AnalyzeResponse)
async def analyze_file(request: Dict[str, str] = Body(...)):
    filename = request.get("filename")
    if not filename:
        logger.warning("Analyze request missing filename")
        raise HTTPException(status_code=400, detail="Filename is required")
    file_path = UPLOAD_DIR / filename
    if not file_path.exists():
        logger.warning(f"File not found for analysis: {filename}")
        raise HTTPException(status_code=404, detail="File not found")
    try:
        text = extract_text_from_file(str(file_path))
        if not text:
            logger.warning(f"No text extracted from file: {filename}")
            raise HTTPException(status_code=400, detail="No text extracted from file")
        chunks = chunk_text(text)
        if not chunks:
            logger.warning(f"No chunks generated for file: {filename}")
            raise HTTPException(status_code=400, detail="No chunks generated")
        embeddings = embed_chunks(chunks)
        try:
            with get_db() as conn:
                store_embeddings(chunks, embeddings, filename, conn)
        except psycopg2.Error as e:
            logger.error(f"Database error while storing embeddings for {filename}: {str(e)}")
            raise HTTPException(status_code=500, detail=f"Database error: {str(e)}")
        try:
            delete_file(str(file_path))
        except Exception as e:
            logger.warning(f"Failed to delete file {filename}: {str(e)}")
        logger.info(f"File analyzed successfully: {filename}, chunks: {len(chunks)}")
        return AnalyzeResponse(
            filename=filename,
            chunk_count=len(chunks),
            message="File analyzed and embeddings stored"
        )
    except Exception as e:
        logger.error(f"Error analyzing file {filename}: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error analyzing file: {str(e)}")

@router.post("/transcribe", response_model=TranscribeResponse)
async def transcribe_audio(file: UploadFile = File(...)):
    file_ext = os.path.splitext(file.filename)[1].lower()
    if file_ext not in {".webm"}:
        logger.warning(f"Unsupported audio file type: {file_ext}")
        raise HTTPException(status_code=400, detail="Unsupported audio file type. Allowed: WebM")
    try:
        audio_data = await file.read()
        temp_wav = UPLOAD_DIR / f"temp_{int(time.time())}.wav"
        convert_audio_to_wav(audio_data, str(temp_wav))
        transcribed_text = speech_to_text_streaming(str(temp_wav), chunk_duration_ms=1000)
        # Chunk and store transcribed text
        if transcribed_text and transcribed_text != "No speech detected.":
            chunks = chunk_text(transcribed_text)
            embeddings = embed_chunks(chunks)
            with get_db() as conn:
                store_embeddings(chunks, embeddings, f"audio_{int(time.time())}.webm", conn)
            logger.info(f"Stored {len(chunks)} chunks from transcribed audio: {transcribed_text[:50]}...")
        delete_file(str(temp_wav))
        logger.info(f"Audio transcribed successfully: {transcribed_text}")
        return TranscribeResponse(transcribed_text=transcribed_text)
    except Exception as e:
        logger.error(f"Error transcribing audio: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error transcribing audio: {str(e)}")

@router.post("/query", response_model=QueryResponse)
async def query(request: QueryRequest = Body(...)):
    query_text = request.query
    voice_model = request.voice_model
    logger.info(f"Received query with voice_model: {voice_model}")
    if not query_text:
        logger.warning("Query request missing query text")
        raise HTTPException(status_code=400, detail="Query text is required")
    try:
        start_time = time.time()
        query_embedding = embed_chunks([query_text])[0]
        try:
            with get_db() as conn:
                cursor = conn.cursor()
                cursor.execute(
                    """
                    SELECT chunk_text, document_name, embedding <=> CAST(%s AS vector) AS cosine_distance
                    FROM chunks
                    ORDER BY embedding <=> CAST(%s AS vector)
                    LIMIT 5;
                    """,
                    (query_embedding, query_embedding)
                )
                results = cursor.fetchall()
                top_chunks = [row["chunk_text"] for row in results]
                chunk_metadata = [
                    {"text": row["chunk_text"], "document_name": row["document_name"], "cosine_distance": row["cosine_distance"]}
                    for row in results
                ]
        except psycopg2.Error as e:
            logger.error(f"Database error during vector search: {str(e)}")
            raise HTTPException(status_code=500, detail=f"Database error: {str(e)}")
        if not top_chunks:
            logger.info("No relevant chunks found for query")
            audio_filename = f"no_chunks_{int(time.time())}.wav"
            audio_path = AUDIO_DIR / audio_filename
            text_to_speech("No relevant chunks found in the database.", str(audio_path), voice_model)
            audio_url = f"/audio/{audio_filename}"
            cleanup_old_audio_files()
            return QueryResponse(response="No relevant chunks found", audio_url=audio_url)
        logger.info(f"Retrieved {len(top_chunks)} chunks in {time.time() - start_time:.3f}s")
        for meta in chunk_metadata:
            logger.info(f"Chunk from {meta['document_name']}: {meta['text'][:50]}... (cosine distance: {meta['cosine_distance']:.4f})")
        response = handle_query(query_text, top_chunks, chunk_metadata)
        audio_filename = f"response_{int(time.time())}.wav"
        audio_path = AUDIO_DIR / audio_filename
        text_to_speech(response, str(audio_path), voice_model)
        audio_url = f"/audio/{audio_filename}"
        logger.info(f"Query processed successfully: {query_text} in {time.time() - start_time:.3f}s")
        cleanup_old_audio_files()
        return QueryResponse(response=response, audio_url=audio_url)
    except Exception as e:
        logger.error(f"Error processing query: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error processing query: {str(e)}")

@router.get("/audio/{filename}")
async def get_audio(filename: str):
    audio_path = AUDIO_DIR / filename
    if not audio_path.exists():
        logger.warning(f"Audio file not found: {filename}")
        raise HTTPException(status_code=404, detail="Audio file not found")
    return FileResponse(audio_path, media_type="audio/wav", headers={"Access-Control-Allow-Origin": "*"})

@router.post("/clear", response_model=ClearResponse)
async def clear_data():
    try:
        with get_db() as conn:
            cursor = conn.cursor()
            cursor.execute("TRUNCATE TABLE chunks;")
            conn.commit()
            logger.info("Database table 'chunks' truncated successfully")
        return ClearResponse(message="Database cleared successfully")
    except psycopg2.Error as e:
        logger.error(f"Database error while clearing data: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Database error: {str(e)}")
    except Exception as e:
        logger.error(f"Error clearing data: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error clearing data: {str(e)}")

@router.post("/clear_audio", response_model=ClearResponse)
async def clear_audio_files():
    try:
        for audio_file in AUDIO_DIR.glob("*.wav"):
            os.remove(audio_file)
            logger.info(f"Deleted audio file: {audio_file}")
        logger.info("All audio files cleared successfully")
        return ClearResponse(message="Audio files cleared successfully")
    except Exception as e:
        logger.error(f"Error clearing audio files: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error clearing audio files: {str(e)}")

@router.websocket("/ws/transcribe")
async def websocket_transcribe(websocket: WebSocket):
    await websocket.accept()
    logger.info("WebSocket connection established for transcription")
    try:
        while True:
            # Receive audio chunk (WebM)
            audio_chunk = await websocket.receive_bytes()
            logger.info(f"Received audio chunk of size: {len(audio_chunk)} bytes")
            if not audio_chunk:
                logger.warning("Empty audio chunk received")
                continue
            # Process audio chunk and get transcription
            transcription = await speech_to_text_websocket(audio_chunk)
            # Send transcription back to client
            await websocket.send_json({"transcription": transcription})
            logger.info(f"Sent transcription: {transcription}")
    except Exception as e:
        logger.error(f"WebSocket error: {str(e)}")
        await websocket.close()
        logger.info("WebSocket connection closed")

import os
import time
import shutil
import logging
import json
import io
import wave
import numpy as np
from pathlib import Path
from typing import List
from pydub import AudioSegment
from vosk import Model, KaldiRecognizer
from TTS.api import TTS
import torch.serialization
import collections
import asyncio
import ffmpeg

logger = logging.getLogger(__name__)

UPLOAD_DIR = Path("uploads")
UPLOAD_DIR.mkdir(exist_ok=True)

AUDIO_DIR = Path("audio")
AUDIO_DIR.mkdir(exist_ok=True)

VOSK_MODEL_PATH = r"D:\Projects\OCR_with_PostgresSQL\query_sql\models\vosk-model-en-in-0.5"
if not os.path.exists(VOSK_MODEL_PATH):
    logger.error(f"Vosk model not found at {VOSK_MODEL_PATH}")
    raise FileNotFoundError(f"Vosk model not found at {VOSK_MODEL_PATH}")
vosk_model = Model(VOSK_MODEL_PATH)
logger.info("Vosk model loaded successfully")

VOICE_MODELS = {
    "female": {"model_name": "tts_models/en/ljspeech/tacotron2-DDC", "speaker": None},
    "male": {"model_name": "tts_models/en/vctk/vits", "speaker": "p239"},
}

espeak_path = r"C:\Program Files\eSpeak NG"
if os.path.exists(espeak_path):
    os.environ["PATH"] = espeak_path + os.pathsep + os.environ.get("PATH", "")
    logger.info(f"Added {espeak_path} to PATH for espeak-ng")
else:
    logger.warning(f"espeak-ng path {espeak_path} not found. VITS models may fail to load.")

try:
    from TTS.utils.radam import RAdam
    torch.serialization.add_safe_globals([RAdam, collections.defaultdict, dict])
    logger.info("Added TTS.utils.radam.RAdam, collections.defaultdict, and dict to PyTorch safe globals")
except ImportError:
    logger.error("Failed to import TTS.utils.radam.RAdam for allowlisting")
    raise

TTS_MODEL_DIR = Path("C:/Users/ASUS/AppData/Local/tts")

def clean_tts_model_directory(model_name: str):
    model_dir = TTS_MODEL_DIR / model_name.replace('/', '--')
    if model_dir.exists():
        try:
            shutil.rmtree(model_dir)
            logger.info(f"Cleaned up TTS model directory: {model_dir}")
        except Exception as e:
            logger.warning(f"Failed to clean up TTS model directory {model_dir}: {str(e)}")

UNUSED_MODELS = [
    "tts_models/en/jenny/jenny",
    "tts_models/en/ljspeech/glow-tts",
    "tts_models/en/ek1/tacotron2",
    "tts_models/en/ljspeech/fast_pitch",
]
for model_name in UNUSED_MODELS:
    clean_tts_model_directory(model_name)

TTS_MODELS = {}
try:
    for voice_label, config in VOICE_MODELS.items():
        model_name = config["model_name"]
        if model_name not in TTS_MODELS:
            model_dir = TTS_MODEL_DIR / model_name.replace('/', '--')
            if model_dir.exists():
                logger.info(f"TTS model directory exists: {model_dir}")
            else:
                logger.info(f"TTS model directory not found, will download: {model_dir}")
            try:
                logger.info(f"Attempting to load TTS model: {model_name}")
                TTS_MODELS[model_name] = TTS(model_name=model_name, progress_bar=False)
                logger.info(f"Successfully loaded TTS model: {model_name}")
            except Exception as e:
                logger.error(f"Failed to load TTS model {model_name}: {str(e)}")
                clean_tts_model_directory(model_name)
                if model_name != "tts_models/en/ljspeech/tacotron2-DDC":
                    logger.info(f"Falling back to default TTS model for {voice_label}")
                    TTS_MODELS[model_name] = TTS_MODELS.get("tts_models/en/ljspeech/tacotron2-DDC")
                else:
                    raise
except Exception as e:
    logger.error(f"Critical failure in loading TTS models: {str(e)}")
    raise

def convert_audio_to_wav(audio_data: bytes, output_path: str) -> None:
    temp_webm = None
    try:
        start_time = time.time()
        if not audio_data:
            logger.error("Audio data is empty")
            raise ValueError("Audio data is empty")
        temp_webm = UPLOAD_DIR / f"temp_audio_{int(time.time())}.webm"
        logger.info(f"Writing audio data to {temp_webm}")
        with open(temp_webm, "wb") as f:
            f.write(audio_data)
        if not os.path.exists(temp_webm) or os.path.getsize(temp_webm) == 0:
            logger.error("Temporary WebM file is empty or not created")
            raise ValueError("Temporary WebM file is empty or not created")
        logger.info(f"Converting {temp_webm} to WAV")
        audio = AudioSegment.from_file(temp_webm, format="webm")
        audio = audio.set_frame_rate(16000).set_channels(1).set_sample_width(2)
        logger.info(f"Exporting audio to {output_path}")
        audio.export(output_path, format="wav")
        logger.info(f"Successfully converted to WAV: {output_path} in {time.time() - start_time:.3f}s")
        os.remove(temp_webm)
        logger.info(f"Deleted temporary WebM file: {temp_webm}")
    except Exception as e:
        logger.error(f"Error converting audio to WAV: {str(e)}")
        if temp_webm and os.path.exists(temp_webm):
            os.remove(temp_webm)
            logger.info(f"Cleaned up temporary WebM file: {temp_webm}")
        raise

async def speech_to_text_websocket(audio_chunk: bytes) -> str:
    try:
        start_time = time.time()
        logger.info(f"Processing WebSocket audio chunk of size: {len(audio_chunk)} bytes")
        if not audio_chunk:
            logger.warning("Empty audio chunk received")
            return ""

        # Save chunk for debugging
        debug_webm = UPLOAD_DIR / f"debug_chunk_{int(time.time())}.webm"
        with open(debug_webm, "wb") as f:
            f.write(audio_chunk)
        logger.info(f"Saved debug chunk: {debug_webm}")

        # Try FFmpeg directly to validate WebM
        try:
            probe = ffmpeg.probe(debug_webm)
            logger.info(f"FFmpeg probe: {probe}")
        except ffmpeg.Error as e:
            logger.error(f"FFmpeg probe failed: {e.stderr.decode()}")
            os.remove(debug_webm)
            return ""

        # Convert WebM to WAV using pydub
        try:
            audio = AudioSegment.from_file(debug_webm, format="webm")
            audio = audio.set_frame_rate(16000).set_channels(1).set_sample_width(2)
            chunk_file = io.BytesIO()
            audio.export(chunk_file, format="wav")
            chunk_file.seek(0)
        except Exception as e:
            logger.error(f"Error converting WebM to WAV with pydub: {str(e)}")
            os.remove(debug_webm)
            return ""

        # Transcribe WAV chunk
        recognizer = KaldiRecognizer(vosk_model, 16000)
        recognizer.SetWords(True)
        transcribed_text = ""

        with wave.open(chunk_file, 'rb') as wf:
            logger.info(f"Chunk parameters: channels={wf.getnchannels()}, sample_width={wf.getsampwidth()}, framerate={wf.getframerate()}")
            if wf.getnchannels() != 1 or wf.getsampwidth() != 2 or wf.getframerate() != 16000:
                logger.error("Invalid chunk: must be mono, 16-bit, 16 kHz")
                os.remove(debug_webm)
                return ""
            while True:
                data = wf.readframes(4000)
                if len(data) == 0:
                    break
                if recognizer.AcceptWaveform(data):
                    result = recognizer.Result()
                    logger.info(f"Raw Vosk result: {result}")
                    try:
                        text = json.loads(result).get("text", "")
                        if text:
                            logger.info(f"Transcribed chunk: {text}")
                            transcribed_text += text + " "
                    except json.JSONDecodeError as e:
                        logger.error(f"Error parsing Vosk result: {str(e)}")
            final_result = recognizer.FinalResult()
            logger.info(f"Final Vosk result: {final_result}")
            try:
                text = json.loads(final_result).get("text", "")
                if text:
                    logger.info(f"Final transcribed chunk: {text}")
                    transcribed_text += text
            except json.JSONDecodeError as e:
                logger.error(f"Error parsing final Vosk result: {str(e)}")

        transcribed_text = transcribed_text.strip()
        logger.info(f"WebSocket transcription completed: {transcribed_text} in {time.time() - start_time:.3f}s")
        os.remove(debug_webm)
        logger.info(f"Deleted debug chunk: {debug_webm}")
        return transcribed_text if transcribed_text else ""
    except Exception as e:
        logger.error(f"Error in WebSocket speech-to-text: {str(e)}")
        if 'debug_webm' in locals() and os.path.exists(debug_webm):
            os.remove(debug_webm)
            logger.info(f"Cleaned up debug chunk: {debug_webm}")
        return ""

def speech_to_text(audio_path: str) -> str:
    try:
        start_time = time.time()
        logger.info(f"Opening WAV file: {audio_path}")
        wf = wave.open(audio_path, "rb")
        logger.info(f"WAV file parameters: channels={wf.getnchannels()}, sample_width={wf.getsampwidth()}, framerate={wf.getframerate()}")
        if wf.getnchannels() != 1 or wf.getsampwidth() != 2 or wf.getframerate() != 16000:
            raise ValueError(f"Audio file {audio_path} must be WAV format, mono, 16-bit, 16 kHz")
        recognizer = KaldiRecognizer(vosk_model, wf.getframerate())
        recognizer.SetWords(True)
        transcribed_text = ""
        chunk_count = 0
        while True:
            data = wf.readframes(4000)
            if len(data) == 0:
                break
            chunk_count += 1
            logger.info(f"Processing audio chunk {chunk_count}")
            if recognizer.AcceptWaveform(data):
                result = recognizer.Result()
                logger.info(f"Raw Vosk result: {result}")
                try:
                    text = json.loads(result).get("text", "")
                    logger.info(f"Transcribed chunk {chunk_count}: {text}")
                    transcribed_text += text + " "
                except json.JSONDecodeError as e:
                    logger.error(f"Error parsing Vosk result for chunk {chunk_count}: {str(e)}")
        final_result = recognizer.FinalResult()
        logger.info(f"Final Vosk result: {final_result}")
        try:
            text = json.loads(final_result).get("text", "")
            logger.info(f"Transcribed final chunk: {text}")
            transcribed_text += text
        except json.JSONDecodeError as e:
            logger.error(f"Error parsing final Vosk result: {str(e)}")
        transcribed_text = transcribed_text.strip()
        wf.close()
        logger.info(f"Transcription completed: {transcribed_text} in {time.time() - start_time:.3f}s")
        return transcribed_text if transcribed_text else "No speech detected."
    except Exception as e:
        logger.error(f"Error in speech-to-text: {str(e)}")
        if 'wf' in locals():
            wf.close()
        raise

def speech_to_text_streaming(audio_path: str, chunk_duration_ms: int = 1000) -> str:
    try:
        start_time = time.time()
        logger.info(f"Loading WAV file for streaming: {audio_path}")
        audio = AudioSegment.from_wav(audio_path)
        audio_duration = len(audio) / 1000.0
        logger.info(f"Audio duration: {audio_duration} seconds")
        chunk_length_ms = chunk_duration_ms
        transcribed_text = ""
        chunk_count = 0
        recognizer = KaldiRecognizer(vosk_model, 16000)
        recognizer.SetWords(True)
        for i in range(0, len(audio), chunk_length_ms):
            chunk_count += 1
            chunk = audio[i:i + chunk_length_ms]
            chunk_duration = len(chunk) / 1000.0
            logger.info(f"Processing chunk {chunk_count} ({chunk_duration}s)")
            chunk = chunk.set_frame_rate(16000).set_channels(1).set_sample_width(2)
            raw_data = np.array(chunk.get_array_of_samples(), dtype=np.int16).tobytes()
            chunk_file = io.BytesIO()
            with wave.open(chunk_file, 'wb') as wf:
                wf.setnchannels(1)
                wf.setsampwidth(2)
                wf.setframerate(16000)
                wf.writeframes(raw_data)
            chunk_file.seek(0)
            wf = wave.open(chunk_file, 'rb')
            logger.info(f"Chunk {chunk_count} parameters: channels={wf.getnchannels()}, sample_width={wf.getsampwidth()}, framerate={wf.getframerate()}")
            if wf.getnchannels() != 1 or wf.getsampwidth() != 2 or wf.getframerate() != 16000:
                logger.error(f"Chunk {chunk_count} invalid: must be mono, 16-bit, 16 kHz")
                wf.close()
                continue
            chunk_start_time = time.time()
            while True:
                data = wf.readframes(4000)
                if len(data) == 0:
                    break
                if recognizer.AcceptWaveform(data):
                    result = recognizer.Result()
                    logger.info(f"Chunk {chunk_count} raw Vosk result: {result}")
                    try:
                        text = json.loads(result).get("text", "")
                        if text:
                            logger.info(f"Chunk {chunk_count} transcribed: {text}")
                            transcribed_text += text + " "
                    except json.JSONDecodeError as e:
                        logger.error(f"Chunk {chunk_count} error parsing Vosk result: {str(e)}")
            final_result = recognizer.FinalResult()
            logger.info(f"Chunk {chunk_count} final Vosk result: {final_result}")
            try:
                text = json.loads(final_result).get("text", "")
                if text:
                    logger.info(f"Chunk {chunk_count} final transcribed: {text}")
                    transcribed_text += text + " "
            except json.JSONDecodeError as e:
                logger.error(f"Chunk {chunk_count} error parsing final Vosk result: {str(e)}")
            wf.close()
            logger.info(f"Chunk {chunk_count} processed in {time.time() - chunk_start_time:.3f}s")
        transcribed_text = transcribed_text.strip()
        total_time = time.time() - start_time
        logger.info(f"Streaming transcription completed: {transcribed_text} in {total_time:.3f}s")
        return transcribed_text if transcribed_text else "No speech detected."
    except Exception as e:
        logger.error(f"Error in streaming speech-to-text: {str(e)}")
        if 'wf' in locals():
            wf.close()
        raise

def text_to_speech(text: str, output_path: str, voice_model: str = "female") -> None:
    try:
        start_time = time.time()
        logger.info(f"Using voice model in text_to_speech: {voice_model}")
        if voice_model not in VOICE_MODELS:
            logger.warning(f"Invalid voice model: {voice_model}, falling back to default 'female'")
            voice_model = "female"
        config = VOICE_MODELS[voice_model]
        model_name = config["model_name"]
        tts_model = TTS_MODELS.get(model_name)
        if not tts_model:
            logger.error(f"TTS model {model_name} not loaded, falling back to default")
            voice_model = "female"
            config = VOICE_MODELS[voice_model]
            tts_model = TTS_MODELS[config["model_name"]]
        speaker = config["speaker"]
        logger.info(f"Generating TTS with model: {model_name}, speaker: {speaker}")
        if speaker:
            tts_model.tts_to_file(text=text, file_path=output_path, speaker=speaker)
        else:
            tts_model.tts_to_file(text=text, file_path=output_path, speaker_wav=None)
        logger.info(f"Generated speech saved to {output_path} with voice model {voice_model} in {time.time() - start_time:.3f}s")
    except Exception as e:
        logger.error(f"Error in text-to-speech with voice model {voice_model}: {str(e)}")
        raise

def cleanup_old_audio_files(max_age_seconds=3600):
    try:
        start_time = time.time()
        current_time = time.time()
        for audio_file in AUDIO_DIR.glob("*.wav"):
            file_age = current_time - os.path.getmtime(audio_file)
            if file_age > max_age_seconds:
                os.remove(audio_file)
                logger.info(f"Deleted old audio file: {audio_file}")
        logger.info(f"Cleanup completed in {time.time() - start_time:.3f}s")
    except Exception as e:
        logger.error(f"Error cleaning up audio files: {str(e)}")

from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from pathlib import Path
import os
import logging
from dotenv import load_dotenv
from db import init_db
from routes import router

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    handlers=[
        logging.FileHandler("app.log"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

load_dotenv()

app = FastAPI(
    title="QuerySQL Backend",
    description="A FastAPI backend for document processing and querying.",
    version="1.0.0"
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

UPLOAD_DIR = Path("uploads")
UPLOAD_DIR.mkdir(exist_ok=True)

AUDIO_DIR = Path("audio")
AUDIO_DIR.mkdir(exist_ok=True)

app.include_router(router)

@app.on_event("startup")
async def startup_event():
    try:
        init_db()
        logger.info("Database initialized successfully")
    except Exception as e:
        logger.error(f"Failed to initialize database: {str(e)}")
        raise

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)

And the original code which was working good without websocket are:

